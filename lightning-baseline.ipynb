{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PTYTORCH LIGHTNING BASELINE - UNET**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture\n",
    "\n",
    "Type info here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Support\n",
    "try: \n",
    "    import pydicom as dcm\n",
    "except:\n",
    "    # Use try except for Google Colab\n",
    "    !pip install pydicom\n",
    "from pydicom.data import get_testdata_files\n",
    "import xml\n",
    "import xml.etree.ElementTree as ET \n",
    "\n",
    "# Base\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import gc\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Polygon\n",
    "from matplotlib.path import Path\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.cm import ScalarMappable\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "# SK-learn\n",
    "import sklearn\n",
    "\n",
    "# Files\n",
    "import os\n",
    "from os.path import join, split\n",
    "from glob import glob\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Conv2d, MaxPool2d, ReLU, ConvTranspose2d\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import torchvision\n",
    "from torchvision.transforms import CenterCrop\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "import tempfile\n",
    "import torch.multiprocessing as mp\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed.fsdp import CPUOffload, wrap\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "from pytorch_lightning import Trainer\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    random_seed = 42\n",
    "    gated = True\n",
    "    path = \"Coronary CT Data\\Gated_release_final\" if gated else \"Coronary CT Data/deidentified_nongated\"\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    batch_size = 32\n",
    "    nEpochs = 2\n",
    "    lr = 0.01\n",
    "\n",
    "    TH = 0.5\n",
    "\n",
    "    model_name = \"lightning-baseline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/753 [1:17:32<?, ?it/s]\n",
      "Epoch 0:   0%|          | 2/753 [1:15:48<474:23:47,  0.00it/s, v_num=5, train_loss_step=0.000]\n",
      "Epoch 0:   0%|          | 0/753 [1:00:57<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6056"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "\n",
    "    # initialize the process group\n",
    "    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 42\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed=CFG.random_seed):\n",
    "    print(f\"Seed: {seed}\")\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseXML(xmlfile): \n",
    "    # create element tree object \n",
    "    tree = ET.parse(xmlfile) \n",
    "\n",
    "    all_images = []\n",
    "\n",
    "    images = tree.find(\"dict\").find(\"array\")\n",
    "    images = images.findall(\"dict\")\n",
    "\n",
    "    # Images\n",
    "    for image in images:\n",
    "        image_data = {}\n",
    "        arr = [i.text for i in image if i.tag not in [\"array\", \"dict\"]]\n",
    "        \n",
    "        for i in range(len(arr)//2):\n",
    "            image_data[arr[2*i]] = arr[2*i+1]\n",
    "\n",
    "        image_data['ROIs'] = []\n",
    "\n",
    "        # ROI\n",
    "        all_roi = image.find(\"array\").findall('dict')\n",
    "        for roi in all_roi:\n",
    "            roi_data = {}\n",
    "            arr = [i.text for i in roi if i.tag not in [\"array\", \"dict\"]]\n",
    "        \n",
    "            for i in range(len(arr)//2):\n",
    "                roi_data[arr[2*i]] = arr[2*i+1]\n",
    "\n",
    "            all_points = roi.findall('array')\n",
    "            roi_data['point_mm'] = [i.text for i in all_points[0].findall(\"string\")]\n",
    "            roi_data['point_px'] = [i.text for i in all_points[1].findall(\"string\")]\n",
    "            \n",
    "            image_data['ROIs'].append(roi_data)\n",
    "        all_images.append(image_data)\n",
    "\n",
    "    return all_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_segments(image_array, points):\n",
    "    polygon = Polygon(points, closed=True, edgecolor='r', facecolor='r')\n",
    "    polygon_indices = np.array(points)\n",
    "    polygon_indices[:, 0] = np.clip(polygon_indices[:, 0], 0, 511)\n",
    "    polygon_indices[:, 1] = np.clip(polygon_indices[:, 1], 0, 511)\n",
    "    image_array[polygon_indices[:, 1], polygon_indices[:, 0]] = 1\n",
    "    polygon_path = Path(polygon_indices)\n",
    "    x, y = np.meshgrid(np.arange(512), np.arange(512))\n",
    "    points = np.column_stack((x.flatten(), y.flatten()))\n",
    "    mask = polygon_path.contains_points(points).reshape(512, 512)\n",
    "    image_array[mask] = 1\n",
    "\n",
    "    return image_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTDataset(Dataset):\n",
    "    def __init__(self, dir):\n",
    "        super().__init__()\n",
    "        self.dir = dir\n",
    "        self.images_path = join(dir, \"images\")\n",
    "        self.labels_path = join(dir, \"labels\")\n",
    "        self.images = os.listdir(self.images_path)\n",
    "        self.labels = os.listdir(self.labels_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx): # Return tuple (x, y)\n",
    "        img = np.load(join(self.images_path, self.images[idx]))\n",
    "        img = img.reshape(1, 512, 512) # Hard coded since all images are 512, 512\n",
    "\n",
    "        label = np.load(join(self.labels_path, self.labels[idx]))\n",
    "        label = label.reshape(1, 512, 512)\n",
    "        return  img,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = CTDataset(\"Numpy Dataset\\\\train\")\n",
    "valid = CTDataset(\"Numpy Dataset\\\\valid\")\n",
    "test = CTDataset(\"Numpy Dataset\\\\test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDL = DataLoader(train, batch_size=CFG.batch_size,shuffle=True)\n",
    "validDL = DataLoader(valid, batch_size=CFG.batch_size)\n",
    "testDL  = DataLoader(test , batch_size=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 24068\n",
      "Valid: 12034\n",
      "Test: 4011\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train: {len(train)}\")\n",
    "print(f\"Valid: {len(valid)}\")\n",
    "print(f\"Test: {len(test)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = Conv2d(in_ch, out_ch, 3)\n",
    "        self.relu  = ReLU()\n",
    "        self.conv2 = Conv2d(out_ch, out_ch, 3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.relu(self.conv2(self.relu(self.conv1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, channels = (1,32,64,128,256,512,1024)):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.pool = MaxPool2d((2,2))\n",
    "\n",
    "        self.encoder = nn.ModuleList([\n",
    "            Block(channels[i], channels[i+1]) for i in range(len(channels)-1) # 1, 32, ..., 1024\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_out = []\n",
    "        for block in self.encoder: # Goes through all blocks, passes through block and saves skip output\n",
    "            x = block(x)\n",
    "            skip_out.append(x)\n",
    "            x = self.pool(x) # Reduces dim\n",
    "        return skip_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoDecoder(nn.Module):\n",
    "    def __init__(self, channels = (1,32,64,128,256,512,1024)):\n",
    "        super().__init__()\n",
    "        self.channels = channels[:0:-1] # Reverse of Encoder (Excluding First Unneeded in Output)\n",
    "        self.pool = MaxPool2d((2,2))\n",
    "        self.upconv = nn.ModuleList([\n",
    "            ConvTranspose2d(self.channels[i], self.channels[i+1], 2, 2) for i in range(len(self.channels)-1)\n",
    "        ])\n",
    "\n",
    "        self.decoder = nn.ModuleList([\n",
    "            Block(self.channels[i], self.channels[i+1]) for i in range(len(self.channels)-1)\n",
    "        ])\n",
    "\n",
    "    def center_crop(self, x, enc_out): # Crop encoder output\n",
    "        _, _, h, w = x.shape\n",
    "        enc_out = CenterCrop([h,w])(enc_out)\n",
    "        return enc_out\n",
    "    \n",
    "    def forward(self, x, enc_out:list):\n",
    "        for i in range(len(self.channels)-1):\n",
    "            x = self.upconv[i](x)\n",
    "            enc_ftrs = self.center_crop(x, enc_out[i]) # Crop Skip\n",
    "            x = torch.cat([x, enc_ftrs], dim=1) # Concatenate Decoder and Skip\n",
    "            x = self.decoder[i](x)\n",
    "\n",
    "            # Min Max Scaling [0,1]\n",
    "            x = (x-x.min())/(x.max()-x.min())\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNET(nn.Module):\n",
    "    def __init__(self, channels = (1,32,64,128,256,512,1024)):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder Path\n",
    "        self.enc_path = AutoEncoder(channels)\n",
    "\n",
    "        # Decoder Path\n",
    "        self.dec_path = AutoDecoder(channels)\n",
    "\n",
    "        self.out = Conv2d(channels[1], 1, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        skips = self.enc_path(x)\n",
    "        x = self.dec_path(skips[::-1][0], skips[::-1][1:]) \n",
    "        # Reverse of enc_out = upward path of decoder \n",
    "        #  [0] -> 1024 output\n",
    "        # [1:] -> All other skip outputs\n",
    "        x = self.out(x)\n",
    "        x = F.interpolate(x, (512,512))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNET().to(CFG.device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lightning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sørensen–Dice coefficient:\n",
    "[Wikipedia](https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient)\n",
    "</br>\n",
    "$${\\displaystyle DSC={\\frac {2|X\\cap Y|}{|X|+|Y|}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningModel(pl.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def loss_fn(self, pred, target):\n",
    "        print(pred.shape)\n",
    "        print(target.shape)\n",
    "        pred = (pred>CFG.TH) # Covnert to 0s and 1s based on threshold\n",
    "        target = target\n",
    "        intersection = int((pred*target).sum())\n",
    "        numerator = 2 * intersection\n",
    "        denominator = pred.sum() + target.sum() # X + Y\n",
    "        denominator = denominator if denominator!=0 else 1\n",
    "        return torch.Tensor(numerator/denominator)\n",
    "    \n",
    "    def _common_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x, y = x.to(CFG.device), y.to(CFG.device)\n",
    "        pred = self.forward(x)\n",
    "        loss = self.loss_fn(pred, y)\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._common_step(batch, batch_idx)\n",
    "        loss.requires_grad = True\n",
    "        # loss.backward() # Check Later\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self._common_step(batch, batch_idx)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss = self._common_step(batch, batch_idx)\n",
    "        self.log(\"test_loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x, y = x.to(CFG.device), y.to(CFG.device)\n",
    "        pred = self.forward(x)\n",
    "        return pred>CFG.TH\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.model.parameters(), lr=CFG.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightning_model = LightningModel(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(accelerator=\"gpu\", devices=[0], min_epochs=1, max_epochs=CFG.nEpochs, precision=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | UNET | 31.1 M\n",
      "-------------------------------\n",
      "31.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "31.1 M    Total params\n",
      "124.376   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/753 [00:00<?, ?it/s] torch.Size([32, 1, 512, 512])\n",
      "torch.Size([32, 1, 512, 512])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "No inf checks were recorded for this optimizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[102], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(lightning_model, trainDL)\n",
      "File \u001b[1;32mc:\\Users\\chewr\\anaconda3\\envs\\tf\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:543\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    541\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m TrainerStatus\u001b[39m.\u001b[39mRUNNING\n\u001b[0;32m    542\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 543\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[0;32m    544\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[0;32m    545\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\chewr\\anaconda3\\envs\\tf\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     43\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m---> 44\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32mc:\\Users\\chewr\\anaconda3\\envs\\tf\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:579\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    572\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    573\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    574\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[0;32m    575\u001b[0m     ckpt_path,\n\u001b[0;32m    576\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    577\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    578\u001b[0m )\n\u001b[1;32m--> 579\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[0;32m    581\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[0;32m    582\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\chewr\\anaconda3\\envs\\tf\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:986\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    981\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[0;32m    983\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    984\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 986\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[0;32m    988\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    989\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    990\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    991\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\chewr\\anaconda3\\envs\\tf\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1032\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1030\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_sanity_check()\n\u001b[0;32m   1031\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> 1032\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[0;32m   1033\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1034\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnexpected state \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\chewr\\anaconda3\\envs\\tf\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    204\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start()\n\u001b[1;32m--> 205\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance()\n\u001b[0;32m    206\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[0;32m    207\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\chewr\\anaconda3\\envs\\tf\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_epoch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    362\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_fetcher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 363\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher)\n",
      "File \u001b[1;32mc:\\Users\\chewr\\anaconda3\\envs\\tf\\lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:138\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdone:\n\u001b[0;32m    137\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 138\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(data_fetcher)\n\u001b[0;32m    139\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end(data_fetcher)\n\u001b[0;32m    140\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\chewr\\anaconda3\\envs\\tf\\lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:242\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_batch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    240\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mlightning_module\u001b[39m.\u001b[39mautomatic_optimization:\n\u001b[0;32m    241\u001b[0m         \u001b[39m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[1;32m--> 242\u001b[0m         batch_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mautomatic_optimization\u001b[39m.\u001b[39;49mrun(trainer\u001b[39m.\u001b[39;49moptimizers[\u001b[39m0\u001b[39;49m], batch_idx, kwargs)\n\u001b[0;32m    243\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    244\u001b[0m         batch_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanual_optimization\u001b[39m.\u001b[39mrun(kwargs)\n",
      "File \u001b[1;32mc:\\Users\\chewr\\anaconda3\\envs\\tf\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:191\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[1;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m         closure()\n\u001b[0;32m    186\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \u001b[39m# BACKWARD PASS\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[39m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 191\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer_step(batch_idx, closure)\n\u001b[0;32m    193\u001b[0m result \u001b[39m=\u001b[39m closure\u001b[39m.\u001b[39mconsume_result()\n\u001b[0;32m    194\u001b[0m \u001b[39mif\u001b[39;00m result\u001b[39m.\u001b[39mloss \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\chewr\\anaconda3\\envs\\tf\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:269\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[1;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_progress\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep\u001b[39m.\u001b[39mincrement_ready()\n\u001b[0;32m    268\u001b[0m \u001b[39m# model hook\u001b[39;00m\n\u001b[1;32m--> 269\u001b[0m call\u001b[39m.\u001b[39;49m_call_lightning_module_hook(\n\u001b[0;32m    270\u001b[0m     trainer,\n\u001b[0;32m    271\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39moptimizer_step\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    272\u001b[0m     trainer\u001b[39m.\u001b[39;49mcurrent_epoch,\n\u001b[0;32m    273\u001b[0m     batch_idx,\n\u001b[0;32m    274\u001b[0m     optimizer,\n\u001b[0;32m    275\u001b[0m     train_step_and_backward_closure,\n\u001b[0;32m    276\u001b[0m )\n\u001b[0;32m    278\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m should_accumulate:\n\u001b[0;32m    279\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_progress\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep\u001b[39m.\u001b[39mincrement_completed()\n",
      "File \u001b[1;32mc:\\Users\\chewr\\anaconda3\\envs\\tf\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:157\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[1;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    154\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m hook_name\n\u001b[0;32m    156\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[LightningModule]\u001b[39m\u001b[39m{\u001b[39;00mpl_module\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 157\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    159\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    160\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32mc:\\Users\\chewr\\anaconda3\\envs\\tf\\lib\\site-packages\\pytorch_lightning\\core\\module.py:1303\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[1;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[0;32m   1264\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimizer_step\u001b[39m(\n\u001b[0;32m   1265\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   1266\u001b[0m     epoch: \u001b[39mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1269\u001b[0m     optimizer_closure: Optional[Callable[[], Any]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1270\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1271\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[0;32m   1272\u001b[0m \u001b[39m    the optimizer.\u001b[39;00m\n\u001b[0;32m   1273\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1301\u001b[0m \n\u001b[0;32m   1302\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1303\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep(closure\u001b[39m=\u001b[39;49moptimizer_closure)\n",
      "File \u001b[1;32mc:\\Users\\chewr\\anaconda3\\envs\\tf\\lib\\site-packages\\pytorch_lightning\\core\\optimizer.py:152\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[1;34m(self, closure, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[39mraise\u001b[39;00m MisconfigurationException(\u001b[39m\"\u001b[39m\u001b[39mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    151\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_strategy \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 152\u001b[0m step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_strategy\u001b[39m.\u001b[39moptimizer_step(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer, closure, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    154\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_on_after_step()\n\u001b[0;32m    156\u001b[0m \u001b[39mreturn\u001b[39;00m step_output\n",
      "File \u001b[1;32mc:\\Users\\chewr\\anaconda3\\envs\\tf\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:239\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[1;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[39m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(model, pl\u001b[39m.\u001b[39mLightningModule)\n\u001b[1;32m--> 239\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39moptimizer_step(optimizer, model\u001b[39m=\u001b[39mmodel, closure\u001b[39m=\u001b[39mclosure, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\chewr\\anaconda3\\envs\\tf\\lib\\site-packages\\pytorch_lightning\\plugins\\precision\\amp.py:96\u001b[0m, in \u001b[0;36mMixedPrecision.optimizer_step\u001b[1;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39m# in manual optimization, the closure does not return a value\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m skip_unscaling:\n\u001b[0;32m     95\u001b[0m     \u001b[39m# note: the scaler will skip the `optimizer.step` if nonfinite gradients are found\u001b[39;00m\n\u001b[1;32m---> 96\u001b[0m     step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler\u001b[39m.\u001b[39mstep(optimizer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     97\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler\u001b[39m.\u001b[39mupdate()\n\u001b[0;32m     98\u001b[0m     \u001b[39mreturn\u001b[39;00m step_output\n",
      "File \u001b[1;32mc:\\Users\\chewr\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:368\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[1;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[39mif\u001b[39;00m optimizer_state[\u001b[39m\"\u001b[39m\u001b[39mstage\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mis\u001b[39;00m OptState\u001b[39m.\u001b[39mREADY:\n\u001b[0;32m    366\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munscale_(optimizer)\n\u001b[1;32m--> 368\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(optimizer_state[\u001b[39m\"\u001b[39m\u001b[39mfound_inf_per_device\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    370\u001b[0m retval \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_opt_step(optimizer, optimizer_state, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    372\u001b[0m optimizer_state[\u001b[39m\"\u001b[39m\u001b[39mstage\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m OptState\u001b[39m.\u001b[39mSTEPPED\n",
      "\u001b[1;31mAssertionError\u001b[0m: No inf checks were recorded for this optimizer."
     ]
    }
   ],
   "source": [
    "trainer.fit(lightning_model, trainDL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, f\"Models/{CFG.model_name}.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
